{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ed9ea4-52af-4e04-8555-00fbee1f6a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "print(\"Iniciando Camada Gold - Projeto 1: Logística...\")\n",
    "\n",
    "#Criação do Database Gold\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "\n",
    "\n",
    "print(\"Criando tabela gold.ft_vendas_consumidor_local...\")\n",
    "\n",
    "#Ler as tabelas Silver necessárias\n",
    "df_pedido_total = spark.table(\"silver.pedido_total\")\n",
    "df_consumidores = spark.table(\"silver.ft_consumidores\")\n",
    "\n",
    "#Realizar o Join\n",
    "df_vendas_local = df_pedido_total.join(\n",
    "    df_consumidores,\n",
    "    on=\"id_consumidor\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "#Selecionar e renomear colunas conforme PDF\n",
    "df_ft_vendas_local = df_vendas_local.select(\n",
    "    F.col(\"id_pedido\"),\n",
    "    F.col(\"id_consumidor\"),\n",
    "    F.col(\"valor_total_pago_brl\").alias(\"valor_total_pedido_brl\"),\n",
    "    F.col(\"cidade\"),\n",
    "    F.col(\"estado\"),\n",
    "    F.col(\"data_pedido\")\n",
    ")\n",
    "\n",
    "#Salvar a FATO na camada Gold\n",
    "df_ft_vendas_local.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"gold.ft_vendas_consumidor_local\")\n",
    "\n",
    "print(\"Tabela gold.ft_vendas_consumidor_local criada com sucesso.\")\n",
    "\n",
    "print(\"Criando view gold.view_total_compras_por_consumidor...\")\n",
    "\n",
    "query_view = \"\"\"\n",
    "    SELECT \n",
    "        cidade,\n",
    "        estado,\n",
    "        COUNT(id_pedido) AS quantidade_vendas,\n",
    "        SUM(valor_total_pedido_brl) AS valor_total_localidade\n",
    "    FROM gold.ft_vendas_consumidor_local\n",
    "    GROUP BY cidade, estado\n",
    "\"\"\"\n",
    "\n",
    "#Executa o comando SQL para criar a view\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW gold.view_total_compras_por_consumidor AS {query_view}\")\n",
    "\n",
    "print(\"View gold.view_total_compras_por_consumidor criada com sucesso.\")\n",
    "\n",
    "\n",
    "print(\"--- Resposta da Pergunta de Negócio: Total de Vendas por Estado ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        estado, \n",
    "        SUM(valor_total_localidade) as total_vendas_estado\n",
    "    FROM gold.view_total_compras_por_consumidor\n",
    "    GROUP BY estado\n",
    "    ORDER BY total_vendas_estado DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38f450c-171a-4f4c-a500-736c10bc6b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#logistica_de_atrasos\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Iniciando Camada Gold - Projeto 2: Logística (Atrasos)...\")\n",
    "\n",
    "\n",
    "print(\"Criando tabela gold.ft_atrasos_pedidos_local_vendedor...\")\n",
    "\n",
    "#Carregar tabelas Silver necessárias\n",
    "df_pedidos = spark.table(\"silver.ft_pedidos\")\n",
    "df_consumidores = spark.table(\"silver.ft_consumidores\")\n",
    "df_itens = spark.table(\"silver.ft_itens_pedidos\")\n",
    "\n",
    "#Realizar Joins\n",
    "#Precisamos: Pedido -> Consumidor (para local)\n",
    "#Precisamos: Pedido -> Item (para pegar o id_vendedor)\n",
    "df_join_passo1 = df_pedidos.join(df_consumidores, on=\"id_consumidor\", how=\"inner\")\n",
    "df_completo = df_join_passo1.join(df_itens, on=\"id_pedido\", how=\"inner\")\n",
    "\n",
    "#Selecionar colunas conforme PDF\n",
    "df_ft_atrasos = df_completo.select(\n",
    "    F.col(\"id_pedido\"),\n",
    "    F.col(\"id_vendedor\"),\n",
    "    F.col(\"id_consumidor\"),\n",
    "    F.col(\"entrega_no_prazo\"),\n",
    "    F.col(\"tempo_entrega_dias\"),\n",
    "    F.col(\"tempo_entrega_estimado_dias\"),\n",
    "    F.col(\"cidade\"),\n",
    "    F.col(\"estado\")\n",
    ")\n",
    "\n",
    "#Salvar a FATO\n",
    "df_ft_atrasos.write.mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"gold.ft_atrasos_pedidos_local_vendedor\")\n",
    "\n",
    "print(\"Tabela gold.ft_atrasos_pedidos_local_vendedor criada com sucesso.\")\n",
    "\n",
    "\n",
    "print(\"Criando view gold.view_tempo_medio_entrega_localidade...\")\n",
    "\n",
    "query_view_local = \"\"\"\n",
    "    SELECT \n",
    "        cidade,\n",
    "        estado,\n",
    "        CAST(AVG(tempo_entrega_dias) AS DECIMAL(10,2)) AS tempo_medio_entrega,\n",
    "        CAST(AVG(tempo_entrega_estimado_dias) AS DECIMAL(10,2)) AS tempo_medio_estimado,\n",
    "        CASE \n",
    "            WHEN AVG(tempo_entrega_dias) > AVG(tempo_entrega_estimado_dias) THEN 'SIM'\n",
    "            ELSE 'NÃO'\n",
    "        END AS entrega_maior_que_estimado\n",
    "    FROM gold.ft_atrasos_pedidos_local_vendedor\n",
    "    GROUP BY cidade, estado\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW gold.view_tempo_medio_entrega_localidade AS {query_view_local}\")\n",
    "print(\"View gold.view_tempo_medio_entrega_localidade criada.\")\n",
    "\n",
    "\n",
    "print(\"Criando view gold.view_vendedor_pontualidade...\")\n",
    "\n",
    "query_view_vendedor = \"\"\"\n",
    "    SELECT \n",
    "        id_vendedor,\n",
    "        COUNT(id_pedido) as total_pedidos,\n",
    "        SUM(CASE WHEN entrega_no_prazo = 'Não' THEN 1 ELSE 0 END) as total_atrasados,\n",
    "        CAST(\n",
    "            (SUM(CASE WHEN entrega_no_prazo = 'Não' THEN 1 ELSE 0 END) / COUNT(id_pedido)) * 100 \n",
    "            AS DECIMAL(10,2)\n",
    "        ) as percentual_atraso\n",
    "    FROM gold.ft_atrasos_pedidos_local_vendedor\n",
    "    GROUP BY id_vendedor\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW gold.view_vendedor_pontualidade AS {query_view_vendedor}\")\n",
    "print(\"View gold.view_vendedor_pontualidade criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a58b6d2-9f96-4af9-a9de-048ad8ea5406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#comercial\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, IntegerType, DateType\n",
    "\n",
    "print(\"Iniciando Camada Gold - Projeto 3: Comercial...\")\n",
    "\n",
    "\n",
    "print(\"Criando dimensão gold.dm_tempo...\")\n",
    "\n",
    "#Descobrir o intervalo de datas (do primeiro ao último pedido)\n",
    "min_max_datas = spark.table(\"silver.ft_pedidos\") \\\n",
    "    .select(\n",
    "        F.min(\"pedido_compra_timestamp\").alias(\"data_min\"),\n",
    "        F.max(\"pedido_compra_timestamp\").alias(\"data_max\")\n",
    "    ).first()\n",
    "\n",
    "#Gerar a sequência de datas (sequence + explode)\n",
    "df_datas = spark.sql(f\"\"\"\n",
    "    SELECT explode(sequence(to_date('{min_max_datas.data_min}'), to_date('{min_max_datas.data_max}'), interval 1 day)) AS data\n",
    "\"\"\")\n",
    "\n",
    "#Criar as colunas de calendário (Extração e Tradução)\n",
    "df_dm_tempo = df_datas.select(\n",
    "    F.col(\"data\").alias(\"sk_tempo\"),\n",
    "    F.year(\"data\").alias(\"ano\"),\n",
    "    F.quarter(\"data\").alias(\"trimestre\"),\n",
    "    F.month(\"data\").alias(\"mes\"),\n",
    "    F.weekofyear(\"data\").alias(\"semana_do_ano\"),\n",
    "    F.dayofmonth(\"data\").alias(\"dia\"),\n",
    "    F.dayofweek(\"data\").alias(\"dia_da_semana_num\"), # 1=Domingo, 7=Sábado no padrão Spark padrão\n",
    "    \n",
    "    #Tradução do Nome do Dia\n",
    "    F.when(F.dayofweek(\"data\") == 1, \"Domingo\")\n",
    "     .when(F.dayofweek(\"data\") == 2, \"Segunda-feira\")\n",
    "     .when(F.dayofweek(\"data\") == 3, \"Terça-feira\")\n",
    "     .when(F.dayofweek(\"data\") == 4, \"Quarta-feira\")\n",
    "     .when(F.dayofweek(\"data\") == 5, \"Quinta-feira\")\n",
    "     .when(F.dayofweek(\"data\") == 6, \"Sexta-feira\")\n",
    "     .when(F.dayofweek(\"data\") == 7, \"Sábado\")\n",
    "     .alias(\"dia_da_semana_nome\"),\n",
    "\n",
    "    #Tradução do Nome do Mês\n",
    "    F.when(F.month(\"data\") == 1, \"Janeiro\")\n",
    "     .when(F.month(\"data\") == 2, \"Fevereiro\")\n",
    "     .when(F.month(\"data\") == 3, \"Março\")\n",
    "     .when(F.month(\"data\") == 4, \"Abril\")\n",
    "     .when(F.month(\"data\") == 5, \"Maio\")\n",
    "     .when(F.month(\"data\") == 6, \"Junho\")\n",
    "     .when(F.month(\"data\") == 7, \"Julho\")\n",
    "     .when(F.month(\"data\") == 8, \"Agosto\")\n",
    "     .when(F.month(\"data\") == 9, \"Setembro\")\n",
    "     .when(F.month(\"data\") == 10, \"Outubro\")\n",
    "     .when(F.month(\"data\") == 11, \"Novembro\")\n",
    "     .when(F.month(\"data\") == 12, \"Dezembro\")\n",
    "     .alias(\"mes_nome\"),\n",
    "     \n",
    "    #Flag Fim de Semana (1=Dom, 7=Sab)\n",
    "    F.when(F.dayofweek(\"data\").isin([1, 7]), \"Sim\")\n",
    "     .otherwise(\"Não\")\n",
    "     .alias(\"eh_fim_de_semana\")\n",
    ")\n",
    "\n",
    "#Salvar tabela\n",
    "df_dm_tempo.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"gold.dm_tempo\")\n",
    "print(\"Tabela gold.dm_tempo criada com sucesso.\")\n",
    "\n",
    "\n",
    "print(\"Criando fato gold.ft_vendas_geral...\")\n",
    "\n",
    "#Carregar tabelas Silver\n",
    "df_itens = spark.table(\"silver.ft_itens_pedidos\")\n",
    "df_pedidos = spark.table(\"silver.ft_pedidos\")\n",
    "df_cotacao = spark.table(\"silver.dm_cotacao_dolar\")\n",
    "df_avaliacoes = spark.table(\"silver.ft_avaliacoes_pedidos\")\n",
    "\n",
    "#Preparar Avaliações (Média por pedido, caso haja mais de uma)\n",
    "df_aval_agg = df_avaliacoes.groupBy(\"id_pedido\").agg(\n",
    "    F.avg(\"avaliacao\").cast(DecimalType(3,2)).alias(\"avaliacao_pedido\")\n",
    ")\n",
    "\n",
    "#Realizar os Joins\n",
    "df_join = df_itens.join(df_pedidos, on=\"id_pedido\", how=\"inner\")\n",
    "\n",
    "#Adiciona coluna de data temporária para join com cotação\n",
    "df_join = df_join.withColumn(\"data_ref\", F.to_date(\"pedido_compra_timestamp\"))\n",
    "\n",
    "#Join com Cotação\n",
    "df_join = df_join.join(df_cotacao, df_join.data_ref == df_cotacao.data, how=\"left\")\n",
    "\n",
    "#Join com Avaliação\n",
    "df_join = df_join.join(df_aval_agg, on=\"id_pedido\", how=\"left\")\n",
    "\n",
    "#Calcular valores e Selecionar colunas finais\n",
    "df_ft_vendas_geral = df_join.select(\n",
    "    F.col(\"id_pedido\"),\n",
    "    F.col(\"id_item\"),\n",
    "    F.col(\"id_consumidor\").alias(\"fk_cliente\"),\n",
    "    F.col(\"id_produto\").alias(\"fk_produto\"),\n",
    "    F.col(\"id_vendedor\").alias(\"fk_vendedor\"),\n",
    "    F.col(\"data_ref\").alias(\"fk_tempo\"),\n",
    "    F.col(\"status\").alias(\"status_pedido\"),\n",
    "    F.col(\"tempo_entrega_dias\"),\n",
    "    F.col(\"entrega_no_prazo\"),\n",
    "    \n",
    "    #Valores BRL\n",
    "    F.col(\"preco_BRL\").alias(\"valor_produto_brl\"),\n",
    "    F.col(\"preco_frete\").alias(\"valor_frete_brl\"),\n",
    "    (F.col(\"preco_BRL\") + F.col(\"preco_frete\")).cast(DecimalType(12,2)).alias(\"valor_total_item_brl\"),\n",
    "    \n",
    "    #Valores USD (Valor / Cotacao)\n",
    "    (F.col(\"preco_BRL\") / F.col(\"cotacao_dolar\")).cast(DecimalType(12,2)).alias(\"valor_produto_usd\"),\n",
    "    (F.col(\"preco_frete\") / F.col(\"cotacao_dolar\")).cast(DecimalType(12,2)).alias(\"valor_frete_usd\"),\n",
    "    ((F.col(\"preco_BRL\") + F.col(\"preco_frete\")) / F.col(\"cotacao_dolar\")).cast(DecimalType(12,2)).alias(\"valor_total_item_usd\"),\n",
    "    \n",
    "    F.col(\"cotacao_dolar\"),\n",
    "    F.col(\"avaliacao_pedido\")\n",
    ")\n",
    "\n",
    "#Salvar tabela\n",
    "df_ft_vendas_geral.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"gold.ft_vendas_geral\")\n",
    "print(\"Tabela gold.ft_vendas_geral criada com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae023f2-9b33-472c-b637-7e3b5a29f383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#views_comercial\n",
    "print(\"Iniciando Criação de Views Analíticas - Projeto 3: Comercial...\")\n",
    "\n",
    "print(\"Criando view gold.view_vendas_por_periodo...\")\n",
    "\n",
    "query_periodo = \"\"\"\n",
    "    SELECT \n",
    "        t.ano,\n",
    "        t.trimestre,\n",
    "        t.mes,\n",
    "        t.mes_nome,\n",
    "        t.dia,\n",
    "        t.dia_da_semana_num,\n",
    "        COUNT(DISTINCT f.id_pedido) AS total_pedidos,\n",
    "        COUNT(f.id_item) AS total_itens,\n",
    "        SUM(f.valor_total_item_brl) AS receita_total_brl,\n",
    "        SUM(f.valor_total_item_usd) AS receita_total_usd,\n",
    "        AVG(f.valor_total_item_brl) AS ticket_medio_brl,\n",
    "        AVG(f.avaliacao_pedido) AS avaliacao_media\n",
    "    FROM gold.ft_vendas_geral f\n",
    "    JOIN gold.dm_tempo t ON f.fk_tempo = t.sk_tempo\n",
    "    GROUP BY t.ano, t.trimestre, t.mes, t.mes_nome, t.dia, t.dia_da_semana_num\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW gold.view_vendas_por_periodo AS {query_periodo}\")\n",
    "print(\"View gold.view_vendas_por_periodo criada.\")\n",
    "\n",
    "print(\"--- 1. Dia da semana com maior receita total (BRL) ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT dia_da_semana_num, SUM(receita_total_brl) as receita_acumulada\n",
    "    FROM gold.view_vendas_por_periodo\n",
    "    GROUP BY dia_da_semana_num\n",
    "    ORDER BY receita_acumulada DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"--- 2. Mês com maior ticket médio (BRL) no último ano disponível ---\")\n",
    "#primeiro descobrimos qual é o último ano, depois filtramos\n",
    "spark.sql(\"\"\"\n",
    "    SELECT mes_nome, AVG(ticket_medio_brl) as ticket_medio_mensal\n",
    "    FROM gold.view_vendas_por_periodo\n",
    "    WHERE ano = (SELECT MAX(ano) FROM gold.view_vendas_por_periodo)\n",
    "    GROUP BY mes_nome\n",
    "    ORDER BY ticket_medio_mensal DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "print(\"Criando view gold.view_top_produto...\")\n",
    "\n",
    "#Precisamos juntar a Fato Geral com a Tabela de Produtos (Silver) para pegar nome/peso\n",
    "query_top_prod = \"\"\"\n",
    "    SELECT \n",
    "        p.id_produto,\n",
    "        p.categoria_produto,\n",
    "        COUNT(f.id_item) AS quantidade_vendida,\n",
    "        COUNT(DISTINCT f.id_pedido) AS total_pedidos,\n",
    "        SUM(f.valor_total_item_brl) AS receita_brl,\n",
    "        SUM(f.valor_total_item_usd) AS receita_usd,\n",
    "        AVG(f.valor_produto_brl) AS preco_medio_brl,\n",
    "        AVG(f.avaliacao_pedido) AS avaliacao_media,\n",
    "        AVG(p.peso_produto_gramas) AS peso_medio_gramas\n",
    "    FROM gold.ft_vendas_geral f\n",
    "    JOIN silver.ft_produtos p ON f.fk_produto = p.id_produto\n",
    "    GROUP BY p.id_produto, p.categoria_produto\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW gold.view_top_produto AS {query_top_prod}\")\n",
    "print(\"View gold.view_top_produto criada.\")\n",
    "\n",
    "\n",
    "print(\"Criando view gold.view_vendas_produtos_esteticos...\")\n",
    "\n",
    "#CTE \n",
    "query_fashion = \"\"\"\n",
    "    WITH vendas_fashion AS (\n",
    "        SELECT \n",
    "            t.ano,\n",
    "            t.mes,\n",
    "            p.categoria_produto,\n",
    "            f.id_pedido,\n",
    "            f.id_item,\n",
    "            f.valor_total_item_brl,\n",
    "            f.valor_total_item_usd,\n",
    "            f.avaliacao_pedido\n",
    "        FROM gold.ft_vendas_geral f\n",
    "        JOIN gold.dm_tempo t ON f.fk_tempo = t.sk_tempo\n",
    "        JOIN silver.ft_produtos p ON f.fk_produto = p.id_produto\n",
    "        WHERE p.categoria_produto LIKE 'fashion%'\n",
    "    )\n",
    "    SELECT \n",
    "        ano,\n",
    "        mes,\n",
    "        categoria_produto,\n",
    "        COUNT(DISTINCT id_pedido) AS total_pedidos,\n",
    "        COUNT(id_item) AS total_itens_vendidos,\n",
    "        SUM(valor_total_item_brl) AS receita_total_brl,\n",
    "        SUM(valor_total_item_usd) AS receita_total_usd,\n",
    "        AVG(valor_total_item_brl) AS ticket_medio_brl,\n",
    "        AVG(valor_total_item_usd) AS ticket_medio_usd,\n",
    "        AVG(avaliacao_pedido) AS avaliacao_media\n",
    "    FROM vendas_fashion\n",
    "    GROUP BY ano, mes, categoria_produto\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(f\"CREATE OR REPLACE VIEW gold.view_vendas_produtos_esteticos AS {query_fashion}\")\n",
    "print(\"View gold.view_vendas_produtos_esteticos criada.\")\n",
    "\n",
    "print(\"--- PROJETO 3 CONCLUÍDO E NOTEBOOK GOLD FINALIZADO ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "analise_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
